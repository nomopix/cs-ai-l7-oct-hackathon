{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Covid-19 diagnosis using symptoms\n",
    "\n",
    "[Prateek Gupta](https://www.pgupta.info) \n",
    "\n",
    "2020-05-20\n",
    "\n",
    "**Abstract:** This notebook is a tutorial on building a typical machine learning classifier. \n",
    "The process of doing so spans steps ranging from data wrangling to model selection. \n",
    "With the help of each of these steps we hope to make the reader familiar with challenges involved in building a machine learning system. \n",
    "\n",
    "# Question\n",
    "\n",
    "In this hackathon, we want to build a machine learning model to predict COVID-19 infections from symptoms.\n",
    "It has several applications, for example, triaging patients to be attended to by a doctor or nurse, recommending self-isolation through contact tracing apps. \n",
    "\n",
    "Zoabi et al. [[1]](https://www.nature.com/articles/s41746-020-00372-6) builds a decision tree classifier using the publicly available data reported by the Israeli Ministry of Health.\n",
    "The paper itself dicsusses the various challenges encountered in deploying such a model. \n",
    "It is encouraged to read the paper and learn the challeges and ways to overcome them. \n",
    "\n",
    "However, in this hackathon, we will use their dataset and make the participant familiar with a typical pipeline of building a machine learning system.\n",
    "\n",
    "[1] [Zoabi, Y., Deri-Rozov, S. & Shomron, N. Machine learning-based prediction of COVID-19 diagnosis based on symptoms. npj Digit. Med. 4, 3 (2021).]((https://www.nature.com/articles/s41746-020-00372-6))\n"
   ],
   "metadata": {
    "id": "GFzda9am9vnj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup the workspace\n",
    "\n",
    "We will clone their Git repository to to use their dataset"
   ],
   "metadata": {
    "id": "WfQWCQDjAKN0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import math\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.preprocessing import LabelBinarizer\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.lines import Line2D"
   ],
   "outputs": [],
   "metadata": {
    "id": "35jfRmroICGw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data \n",
    "\n",
    "Let's check how the data looks like and how various features are encoded. "
   ],
   "metadata": {
    "id": "kv6FR_gOAV16"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('data\\corona_tested_individuals_ver_006.english.csv.zip')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kzdw314\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (1,2,3,4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7tANpd5-tqj",
    "outputId": "98096b64-c733-4c43-8cbe-4d3f5abcea3f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.info() # No nulls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278848 entries, 0 to 278847\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   test_date            278848 non-null  object\n",
      " 1   cough                278848 non-null  object\n",
      " 2   fever                278848 non-null  object\n",
      " 3   sore_throat          278848 non-null  object\n",
      " 4   shortness_of_breath  278848 non-null  object\n",
      " 5   head_ache            278848 non-null  object\n",
      " 6   corona_result        278848 non-null  object\n",
      " 7   age_60_and_above     278848 non-null  object\n",
      " 8   gender               278848 non-null  object\n",
      " 9   test_indication      278848 non-null  object\n",
      "dtypes: object(10)\n",
      "memory usage: 21.3+ MB\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXaLQdbBAElY",
    "outputId": "3501d509-7250-496e-bb59-cfc5d9249d81"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.head()\r\n",
    "# We need to one hot encode corona_result, age_60_and_above, gender and test_indication"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    test_date cough fever sore_throat shortness_of_breath head_ache  \\\n",
       "0  2020-04-30     0     0           0                   0         0   \n",
       "1  2020-04-30     1     0           0                   0         0   \n",
       "2  2020-04-30     0     1           0                   0         0   \n",
       "3  2020-04-30     1     0           0                   0         0   \n",
       "4  2020-04-30     1     0           0                   0         0   \n",
       "\n",
       "  corona_result age_60_and_above  gender test_indication  \n",
       "0      negative             None  female           Other  \n",
       "1      negative             None  female           Other  \n",
       "2      negative             None    male           Other  \n",
       "3      negative             None  female           Other  \n",
       "4      negative             None    male           Other  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_date</th>\n",
       "      <th>cough</th>\n",
       "      <th>fever</th>\n",
       "      <th>sore_throat</th>\n",
       "      <th>shortness_of_breath</th>\n",
       "      <th>head_ache</th>\n",
       "      <th>corona_result</th>\n",
       "      <th>age_60_and_above</th>\n",
       "      <th>gender</th>\n",
       "      <th>test_indication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>None</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>None</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>None</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! These are the features used in the paper for their prediction task. The authors also list these features in the [README.md of their Github repo](https://github.com/nshomron/covidpred). "
   ],
   "metadata": {
    "id": "RWHY4TLAAhpJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas read columns as string, so we need to convert them to the proper format before we can operate on it."
   ],
   "metadata": {
    "id": "PEtPMd2qGn72"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df['test_date'] = pd.to_datetime(df['test_date'])"
   ],
   "outputs": [],
   "metadata": {
    "id": "Zi7esn5RGdC8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(\"Start date:\", min(df['test_date']))\r\n",
    "print(\"End date:\", max(df['test_date']))\r\n",
    "\r\n",
    "n_days = (max(df['test_date']) - min(df['test_date'])).days\r\n",
    "print(\"# of days: \", n_days)\r\n",
    "\r\n",
    "n_obs =  df.shape[0]\r\n",
    "print(\"# of observations:\", df.shape[0])\r\n",
    "print(\"# of features:\", df.shape[1])\r\n",
    "\r\n",
    "pos_cases = sum(df['corona_result'] == \"positive\")\r\n",
    "print(\"# of positively diagnosed cases: {0} ({1: 2.2f}%)\".format(pos_cases, 100*pos_cases / n_obs))\r\n",
    "\r\n",
    "neg_cases = sum(df['corona_result'] == \"negative\")\r\n",
    "print(\"# of negatively diagnosed cases: {0} ({1: 2.2f}%)\".format(neg_cases, 100 * neg_cases / n_obs))\r\n",
    "\r\n",
    "other_cases = sum(df['corona_result'] == \"other\") # possibly not confirmed\r\n",
    "print(\"# of other cases (possibly, not confirmed): {0} ({1: 2.2f}%)\".format(other_cases, 100 * other_cases / n_obs))\r\n",
    "\r\n",
    "# Small percentage of positively diagnosed cases - we need to be careful about our accuracy metric. ROC AUC is best."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start date: 2020-03-11 00:00:00\n",
      "End date: 2020-04-30 00:00:00\n",
      "# of days:  50\n",
      "# of observations: 278848\n",
      "# of features: 10\n",
      "# of positively diagnosed cases: 14729 ( 5.28%)\n",
      "# of negatively diagnosed cases: 260227 ( 93.32%)\n",
      "# of other cases (possibly, not confirmed): 3892 ( 1.40%)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mdb3D7Y5BdTt",
    "outputId": "75a9cc98-8a8a-4764-a7df-208cc85202d9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we do not have any information on what happened to \"other\" cases, we will exclude them from our exercise. \n",
    "\n"
   ],
   "metadata": {
    "id": "7guFsw8K-Cvy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df = df[df['corona_result'].isin(['positive', 'negative'])]"
   ],
   "outputs": [],
   "metadata": {
    "id": "I-2eeGvK-Bxv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To build out predictor, we will be splitting our dataset into **training,  validation, and test sets**. \n",
    "A model is trained on the training dataset while the hyperparameters are tuned on the validation dataset. \n",
    "Finally, a test dataset is used to report final model's performance metrics. \n",
    "\n",
    "Since we have a time dependent dataset we will split our training and test dataset based on time. \n",
    "Thus, we find the date before which 60% of observations are present, and use that as our training dataset.\n",
    "We will use next 20% of the dataset as our validation dataset, and finally, the remaining 20% will be used as a test dataset. \n",
    "Thus, we use 60-20-20 split.\n",
    "\n",
    "The authors use 63%-23% training-test split, and a further split of training into train-valid dataset using 80-20% split.\n",
    "There is no prescribed formula on how to do this split. \n",
    "\n"
   ],
   "metadata": {
    "id": "4hG2lfabKr1e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "date_counts = df.groupby(['test_date']).count()['gender'] # take count of any column. They will all be same.\r\n",
    "date_counts = date_counts.sort_index()\r\n",
    "cum_counts = date_counts.cumsum()\r\n",
    "cdf = cum_counts / n_obs\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "s0dEh77THGW6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "max_training_date = cdf[cdf < 0.60].index.max()\r\n",
    "training_data = df[df['test_date'] <= max_training_date]\r\n",
    "\r\n",
    "min_test_date = cdf[cdf > 0.80].index.min()\r\n",
    "test_data = df[df['test_date'] >= min_test_date]\r\n",
    "\r\n",
    "valid_data = df[(max_training_date < df['test_date']) & (df['test_date'] < min_test_date)]\r\n",
    "\r\n",
    "print(\"# of observations in training dataset\", training_data.shape[0])\r\n",
    "print(\"# of observations in validation dataset\", valid_data.shape[0])\r\n",
    "print(\"# of observations in test dataset\", test_data.shape[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of observations in training dataset 160717\n",
      "# of observations in validation dataset 53172\n",
      "# of observations in test dataset 61067\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvTrB-PsMrOv",
    "outputId": "f46dd3b9-4a37-4c62-de4c-d0126ba4d511"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE:**\n",
    "It is **extremely important** that you do not use the test dataset in the model building phase. \n",
    "While building models, it is required to tune the hyperparameter, adjust assumptions, modify features, etc. \n",
    "This should be done on the validation dataset. \n",
    "After several such iterations on the validation dataset, you will pick a model with the best performace as your final model. \n",
    "\n",
    "A test dataset is used to measure the final model's performance, which is a proxy for how it will perform (or generalize) in real life. \n",
    "Thus, to have a proper measure of model's genearalization, test dataset should not be part of your model building process. "
   ],
   "metadata": {
    "id": "J-xJrSsa5_Lz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In this section, we will see the general statistics of features.\n",
    "In doing so, we will encounter inconsistencies in the data and address them accordingly. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "W86CzZGp_rOd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# We want to predict 'corona_result'. \r\n",
    "# We will not use \"test_date\" as a feature. \r\n",
    "# So we narrow down the input features to this list \r\n",
    "INPUT_FEATURES = ['cough', 'fever', 'sore_throat', 'shortness_of_breath', 'head_ache', 'age_60_and_above', 'gender', 'test_indication']\r\n",
    "TARGET_COLUMN = 'corona_result'\r\n",
    "\r\n",
    "for col in INPUT_FEATURES:\r\n",
    "  print(\"*\"*25, f\" {col} \", \"*\"*25)\r\n",
    "  print(training_data[col].value_counts())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*************************  cough  *************************\n",
      "0       119702\n",
      "1        25547\n",
      "0        12039\n",
      "1         3177\n",
      "None       252\n",
      "Name: cough, dtype: int64\n",
      "*************************  fever  *************************\n",
      "0       131874\n",
      "0        13768\n",
      "1        13375\n",
      "1         1448\n",
      "None       252\n",
      "Name: fever, dtype: int64\n",
      "*************************  sore_throat  *************************\n",
      "0       142874\n",
      "0        15973\n",
      "1         1435\n",
      "1          434\n",
      "None         1\n",
      "Name: sore_throat, dtype: int64\n",
      "*************************  shortness_of_breath  *************************\n",
      "0       143279\n",
      "0        15906\n",
      "1         1030\n",
      "1          501\n",
      "None         1\n",
      "Name: shortness_of_breath, dtype: int64\n",
      "*************************  head_ache  *************************\n",
      "0       142294\n",
      "0        16064\n",
      "1         2015\n",
      "1          343\n",
      "None         1\n",
      "Name: head_ache, dtype: int64\n",
      "*************************  age_60_and_above  *************************\n",
      "No      123867\n",
      "Yes      25425\n",
      "None     11425\n",
      "Name: age_60_and_above, dtype: int64\n",
      "*************************  gender  *************************\n",
      "male      72598\n",
      "female    70950\n",
      "None      17169\n",
      "Name: gender, dtype: int64\n",
      "*************************  test_indication  *************************\n",
      "Other                     135059\n",
      "Abroad                     17162\n",
      "Contact with confirmed      8496\n",
      "Name: test_indication, dtype: int64\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jx1mMFLQEMdS",
    "outputId": "b29c246d-3d7e-4a33-8d50-9ed5afa2fdcc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are the questions to guide you through the process of exploring data \n",
    "\n",
    "1.   Think about possible biases and limitations of this dataset\n",
    "2.   What is the format of feature values? Are there any inconsistencies? If so, how would you make them cosistent?\n",
    "3.   What is the statistics of these feature values? How many symptoms are reported or not?\n",
    "4.   Which symptoms have a reporting bias, i.e., likely to be reported when the patient is COVID positive? \n",
    "5.   How will the symptoms with reporting bias affect the model’s performance?\n",
    "6.   Visualization: Draw the bar graph of features grouped by the target class? \n",
    "7.   How does the bar graph of the symptoms with reporting bias looks like?\n",
    "8.   Determine if we have a class imbalance in the dataset? If so, what do you reckon will be the downstream challenges in evaluating the model? How will you overcome those challenges?\n",
    "9.   What does \"None\" value mean for feature? Should we include these features?"
   ],
   "metadata": {
    "id": "P-bVRyzjR-dU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**GOOD PRACTICE**: To make your plots accessible to everyone, it is always a good idea to use colorblind-friendly palette for your plots. Check out [this](https://medium.com/cafe-pixo/inclusive-color-palettes-for-the-web-bbfe8cf2410e) for such a palette."
   ],
   "metadata": {
    "id": "IQRohLR0R8wd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this section, we will transform the features that models can operate upon. Note that this transformation doesn't have to be unique. \n",
    "It is very much dependent on the type of model you are building. \n",
    "\n",
    "Here is the list of questions to guide your feature engineering task \n",
    "\n",
    "1.   How will you represent the features in numerical format that can be accessible by model? \n",
    "2.   Are there any redundancies in your feature representation?\n",
    "3.   How will you represent targets in a format accessible to the model?\n",
    "\n",
    "Check out [`sklearn`'s preprocessing library](https://scikit-learn.org/stable/modules/preprocessing.html) for easy-to-use functions to do this. "
   ],
   "metadata": {
    "id": "GLRMrojV__Se"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def normalize(x):\r\n",
    "  \"\"\"\r\n",
    "  Normalizes the input to integer type. It maps \"None\" to 2.\r\n",
    "  Args:\r\n",
    "    x (int or str): input to be normalized \r\n",
    "  Returns:\r\n",
    "    (int): normalized input\r\n",
    "  \"\"\"\r\n",
    "  if type(x) == str:\r\n",
    "    return int(x) if x != \"None\" else 2\r\n",
    "  return x\r\n",
    "\r\n",
    "def normalize_columns(data):\r\n",
    "  for col in ['cough', 'fever', 'sore_throat', 'shortness_of_breath', 'head_ache']:\r\n",
    "      data[col] = data[col].apply(normalize)\r\n",
    "  return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\r\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\r\n",
    "\r\n",
    "\r\n",
    "def preprocess(data, encoder, lb):\r\n",
    "    \"\"\"\r\n",
    "    Transforms `data` into format required for model building\r\n",
    "    Args:\r\n",
    "      data (pd.DataFrame): dataframe with columns `INPUT_FEATURES` and `TARGET_COLUMN`\r\n",
    "      encoder (sklearn.preprocessing.OneHotEncoder): A fitted OneHotEncoder to be used to transform `INPUT_FEATURES`\r\n",
    "      lb (sklearn.preprocessing.LabelBinarizer): A fitted LabelBinarizer to be used to transform `TARGET_COLUMN`\r\n",
    "    Returns:\r\n",
    "      model_input (np.array): each row is an observation, columns are one-hot encoded features of `INPUT_FEATURES`\r\n",
    "      model_target (np.array): 1D array with 1 where `TARGET_COLUMN` is \"positive\" and 0 otherwise.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Clean values, turning all values into ints to consolidate duplicate values.\r\n",
    "    normalize_columns(data)\r\n",
    "\r\n",
    "    # One hot encode features.\r\n",
    "    model_input = encoder.transform(data[INPUT_FEATURES]).todense()\r\n",
    "    model_target = lb.transform(data[TARGET_COLUMN]).flatten()\r\n",
    "\r\n",
    "    return model_input, model_target\r\n",
    "\r\n",
    "\r\n",
    "# Clean values, turning all values into ints to consolidate duplicate values.\r\n",
    "normalize_columns(training_data)\r\n",
    "\r\n",
    "lb = LabelBinarizer()\r\n",
    "lb.fit(training_data['corona_result'])\r\n",
    "encoder = OneHotEncoder(drop='first').fit(training_data[INPUT_FEATURES]) \r\n",
    "\r\n",
    "X_train, y_train = preprocess(training_data, encoder, lb)\r\n",
    "X_valid, y_valid = preprocess(valid_data, encoder, lb)\r\n",
    "X_test, y_test = preprocess(test_data, encoder, lb)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "cGLQMUmWWKch"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model building\n",
    "\n",
    "In this section, we will build various classifiers using `sklearn`. You do not have to restrict yourself to `sklearn`. Please feel free to use any other library.\n",
    "\n",
    "**TRY:**  Try various classifiers that you have learned so far.\n",
    "Here is the list of models to try :\n",
    "\n",
    "*  Logistic Regssion: [User Guide](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). [API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "*   Decision Trees: [User Guide](https://scikit-learn.org/stable/modules/tree.html#classification). [API](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). You can head down in the User Guide to [other Tree algorithms](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) if you fancy. \n",
    "*   Categorical Naive Bayes.[User Guide](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes). [API](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB)\n",
    "*   Linear Discriminant Analysis. [User Guide](https://scikit-learn.org/stable/modules/lda_qda.html#). [API](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)\n",
    "*   Quadratic Discriminant Analysis. [User Guide](https://scikit-learn.org/stable/modules/lda_qda.html#). [API](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)\n",
    "*   Support Vector Machines. [User Guide](https://scikit-learn.org/stable/modules/svm.html#classification). [API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "*   Nearest neighbors classification. [User Guide](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification). [API](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "*   Neural networks - Multi-layer Perceptron (MLP). [User Guide](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron). [API](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n",
    "\n"
   ],
   "metadata": {
    "id": "5N6PYvo8NUuE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "\r\n",
    "\r\n",
    "# Training baseline models with no hyperparameter tweaking.\r\n",
    "\r\n",
    "lr = LogisticRegression()\r\n",
    "lr.fit(X_train, y_train)\r\n",
    "y_pred_test_lr = lr.predict_proba(X_test)\r\n",
    "roc_auc_score(y_test, y_pred_test_lr[:,1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8015067852096234"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "dtc = DecisionTreeClassifier()\r\n",
    "dtc.fit(X_train, y_train)\r\n",
    "y_pred_test_dtc = dtc.predict_proba(X_test)\r\n",
    "roc_auc_score(y_test, y_pred_test_dtc[:,1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7845554792845546"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "rf = RandomForestClassifier()\r\n",
    "rf.fit(X_train, y_train)\r\n",
    "y_pred_test_rf = rf.predict_proba(X_test)\r\n",
    "roc_auc_score(y_test, y_pred_test_rf[:,1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7845242786958001"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "gnb = GaussianNB()\r\n",
    "gnb.fit(X_train, y_train)\r\n",
    "y_pred_test_bayes = gnb.predict_proba(X_test)\r\n",
    "roc_auc_score(y_test, y_pred_test_bayes[:,1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8117915153991777"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "#knn = KNeighborsClassifier(n_neighbors=19, weights = 'uniform')\r\n",
    "#knn_model = knn.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate Model\n",
    "\n",
    "In this section, we will evaluate our model's performance on the validation dataset. \n",
    "\n",
    "Here are the list of questions to think about while deciding how to evaluate your model - \n",
    "*   Is accuracy the right metric to evaluate the model? Are inaccuracies correctly penalized in the accuracy metric?\n",
    "*  Would you think that the cost of false negative is more than the false positive? Is it dependent on the application?\n",
    "*  Which metric will minimize false negatives and false positives?\n",
    "*   Which dataset should you chose to evaluate the model? Validation or Test?\n",
    "What other metric is relevant in our context?  \n",
    "\n",
    "For benchmarking everyone’s results we will stick to ROC AUC score as a metric. \n",
    "There are standard functions to compute these scores in `sklearn`, so we will use them. \n",
    "Specifically, we will be using [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) and [`plot_roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html).\n"
   ],
   "metadata": {
    "id": "bZSS0TyANfG1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def evaluate(model, X, Y):\r\n",
    "  \"\"\"\r\n",
    "  Returns the AUC-ROC for `model` as evaluated on (X, Y)\r\n",
    "\r\n",
    "  Args:\r\n",
    "    model (): Any model that has a function predict_proba and returns probability for each row in `X`.\r\n",
    "    X (np.array): Input to the model containing feature values\r\n",
    "    Y (np.array): 1D array containing true class i.e. 0 or 1\r\n",
    "  \r\n",
    "  Returns:\r\n",
    "    (float): AUC ROC for the model\r\n",
    "  \"\"\"\r\n",
    "\r\n",
    "  y_score = model.predict(X) # (n_samples, n_clases) with each value being the probability of being in that class\r\n",
    "  return roc_auc_score(Y, y_score)\r\n",
    "\r\n",
    "print('')\r\n",
    "print(f'LR train score: {evaluate(lr, X_train, y_train)}')\r\n",
    "print(f'LR valid score: {evaluate(lr, X_valid, y_valid)}')\r\n",
    "\r\n",
    "print(f'DTC train score: {evaluate(dtc, X_train, y_train)}')\r\n",
    "print(f'DTC valid score: {evaluate(dtc, X_valid, y_valid)}')\r\n",
    "\r\n",
    "print(f'RF train score: {evaluate(rf, X_train, y_train)}')\r\n",
    "print(f'RF valid score: {evaluate(rf, X_valid, y_valid)}')\r\n",
    "\r\n",
    "#£print(f'knn train score: {evaluate(knn, X_train, y_train)}')\r\n",
    "#print(f'knn valid score: {evaluate(knn, X_valid, y_valid)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "LR train score: 0.7498149299178789\n",
      "LR valid score: 0.575557937377261\n",
      "DTC train score: 0.7876597140173531\n",
      "DTC valid score: 0.7835813586382017\n",
      "RF train score: 0.7888820024571146\n",
      "RF valid score: 0.7846228583212276\n"
     ]
    }
   ],
   "metadata": {
    "id": "5TGfy0kYV7pM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Search\n",
    "\n",
    "In this section we will be searching for the best parameters to build our models. \n",
    "This is where we will use our validation dataset. \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "6n0jr-EekESj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameter search can become messy if you have lots of paramters. \n",
    "A brute force method to do such a search will be to do a grid search to fit tons of models. \n",
    "Thus, a smarter way to do hyperparameter search has been the subject of research. \n",
    "\n",
    "**TRY:** If interested, read [here](https://scikit-learn.org/stable/modules/grid_search.html) for more details and incorporate some of those ideas in the model building process. "
   ],
   "metadata": {
    "id": "uqSd5Bo3chPT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\r\n",
    "\r\n",
    "# Grid search for Logistic Regression\r\n",
    "lr_params = {\r\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\r\n",
    "    'penalty': ['l2'],\r\n",
    "    'C': [100, 10, 1.0, 0.1, 0.01]\r\n",
    "}\r\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
    "lr_grid_search = GridSearchCV(estimator=lr, param_grid=lr_params, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\r\n",
    "grid_result = lr_grid_search.fit(X_train, y_train)\r\n",
    "print(grid_result.best_score_)\r\n",
    "print(grid_result.best_params_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_lr = grid_result.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_params = {'n_neighbors': [1,3,5,11,19],\r\n",
    "             'weights': ['uniform','distance'],\r\n",
    "             'metric':['euclidean','manhattan']}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in grid_params['n_neighbors']:\r\n",
    "    for j in grid_params['weights']:\r\n",
    "        for k in grid_params['metric']:\r\n",
    "            \r\n",
    "            knn=KNeighborsClassifier(n_neighbors=i,weights=j,metric=k)\r\n",
    "            knn.fit(x_train, y_train)\r\n",
    "            valid_y_pred=knn.predict(x_eval)\r\n",
    "            # getting a score for test\r\n",
    "            score=roc_auc_score(y_eval,valid_y_pred )\r\n",
    "            print('n_neighbors: ',i,'weights: ',j,'metric: ',k,'roc_auc_score: ',score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Report your results\n",
    "\n",
    "**NOTE:** You should use the test dataset only when you are done with hyperparameter search on your model. \n",
    "This is because the test dataset is not involved in the model building process, thereby making sure that the performance evaluation on the test dataset measures how well the proposed model is able to generalize."
   ],
   "metadata": {
    "id": "UkB6mMuSkkmN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test, Y_test = preprocess(test_data, encoder, lb)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxqeBPIMko9Z",
    "outputId": "b36d385f-50bf-4abe-c447-27861025a9f0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auc_score = evaluate(best_lr, X_test, Y_test)\r\n",
    "print(f\"LR AUC-ROC on the test dataset:{auc_score}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "from numpy import mean\r\n",
    "from numpy import std\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.ensemble import VotingClassifier\r\n",
    "from matplotlib import pyplot\r\n",
    "from sklearn.ensemble import StackingClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.ensemble import BaggingClassifier\r\n",
    "\r\n",
    "models = list()\r\n",
    "models.append(('lr', LogisticRegression(C= 1.0, penalty='l2', solver= 'newton-cg'))) \r\n",
    "models.append(('rf', RandomForestClassifier(max_depth= 5, min_samples_split= 2, n_estimators= 20)))\r\n",
    "models.append(('nb', GaussianNB()))\r\n",
    "\r\n",
    "# define the voting ensemble\r\n",
    "ensemble = StackingClassifier(estimators=models)\r\n",
    "ensemble.fit(X_train, y_train)\r\n",
    "y_score = ensemble.predict_proba(X_test)\r\n",
    "roc_auc_score(y_test, y_score[:,1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.812627131000212"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "M = list()\r\n",
    "M.append(('lr', LogisticRegression(C= 1.0, penalty='l2', solver= 'newton-cg'))) \r\n",
    "M.append(('rf', RandomForestClassifier(max_depth= 5, min_samples_split= 2, n_estimators= 20)))\r\n",
    "M.append(('nb', GaussianNB()))\r\n",
    "E = VotingClassifier(estimators=M, voting='hard')\r\n",
    "ENSEMBLE=E.fit(X_train,y_train)\r\n",
    "ensemble_pred=ENSEMBLE.predict(X_test)\r\n",
    "roc_auc_score(y_test,ensemble_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Report your results (paste AUC curve code here)\n",
    "\n",
    "**NOTE:** You should use the test dataset only when you are done with hyperparameter search on your model. \n",
    "This is because the test dataset is not involved in the model building process, thereby making sure that the performance evaluation on the test dataset measures how well the proposed model is able to generalize."
   ],
   "metadata": {
    "id": "UkB6mMuSkkmN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxqeBPIMko9Z",
    "outputId": "b36d385f-50bf-4abe-c447-27861025a9f0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of COVID-diagnosis-hackathon.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ed2bf37353a0f474d951c84d2fc775e1ceba4935d8b0ef9d3a9862c30075ecf0"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}